{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import globo , os , glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "from dataset import *\n",
    "from utils import *\n",
    "from model import *\n",
    "from loss import RankingLoss\n",
    "from train import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' LOSS FUNCTIONS VERSUS '''\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "lambda1 = lambda2 = 8e-5\n",
    "nrm = tf.zeros([BATCH_SIZE, 32, 1])\n",
    "#abn = tf.ones([BATCH_SIZE, 32, 1])\n",
    "abn = tf.random.uniform([BATCH_SIZE, 32, 1] , maxval = 1)\n",
    "scores = tf.concat((nrm, abn), axis=0)\n",
    "print(scores.shape)\n",
    "\n",
    "\n",
    "def smoothfx(arr , deepmil=False):\n",
    "    if deepmil:\n",
    "        arr2 = np.zeros_like(arr)\n",
    "        arr2[:-1] = arr[1:]\n",
    "        arr2[-1] = arr[-1]\n",
    "    else: \n",
    "        arr2 = tf.concat([arr[1:], arr[-1:]], axis=0)\n",
    "    \n",
    "    loss = tf.reduce_sum(tf.square(arr2 - arr))\n",
    "    return lambda1 * loss\n",
    "\n",
    "\n",
    "def sparsityfx( arr):\n",
    "    loss = tf.reduce_sum(arr)\n",
    "    return lambda1 * loss\n",
    "\n",
    "\n",
    "def loss_deepmil(scores):\n",
    "    print(\"\\n\\nDEEPMIL\")\n",
    "    scores = tf.reshape(scores, (-1, 1))\n",
    "    #print(\"reshape =\",scores.shape)\n",
    "\n",
    "    loss1 = tf.constant(0.0, dtype=tf.float32)\n",
    "    loss2 = tf.constant(0.0, dtype=tf.float32)\n",
    "    \n",
    "    l1 = 0.0 ; l2 = 0.0\n",
    "    for i in range(BATCH_SIZE):\n",
    "        startn = i * 32\n",
    "        endn = (i + 1) * 32\n",
    "        #print(f'startn {startn} , endn {endn}')\n",
    "        \n",
    "        starta = (i * 32 + BATCH_SIZE * 32)\n",
    "        enda = (i + 1) * 32 + BATCH_SIZE * 32\n",
    "        #print(f'starta {starta} , enda {enda}')\n",
    "        \n",
    "        maxn = tf.reduce_max(scores[ startn : endn ])\n",
    "        maxa = tf.reduce_max(scores[ starta : enda ])\n",
    "        #print(maxn,maxn)\n",
    "        \n",
    "        tmp = tf.nn.relu(1.0 - maxa + maxn)\n",
    "        loss1 += tmp\n",
    "        loss1 += sparsityfx(scores[ starta : enda ])\n",
    "        loss1 += smoothfx(scores[ starta : enda ] , False)\n",
    "        \n",
    "        loss2 += tmp\n",
    "        loss2 += sparsityfx(scores[ starta : enda ])\n",
    "        loss2 += smoothfx(scores[ starta : enda ] , True)\n",
    "        \n",
    "    l1 = (loss1 / BATCH_SIZE).numpy()\n",
    "    l2 = (loss2 / BATCH_SIZE).numpy()\n",
    "    print(\"l1\",l1,\"l2\",l2)\n",
    "    print(\"abs dif = \",abs(l1 - l2) , \"\\nisclose?\" ,np.isclose(l1 , l2))\n",
    "    return l1 , l2\n",
    "\n",
    "\n",
    "def loss_milbert(scores):\n",
    "    print(\"\\nMILBERT\")\n",
    "    loss = tf.constant(0.0, dtype=tf.float32)\n",
    "    sparsity = tf.constant(0.0, dtype=tf.float32)\n",
    "    smooth = tf.constant(0.0, dtype=tf.float32)\n",
    "    l=0.0\n",
    "    for i in range(BATCH_SIZE):\n",
    "        #print(i)\n",
    "        normal_index = tf.random.shuffle(tf.range(32))\n",
    "        y_normal  = tf.gather(scores[i], normal_index)\n",
    "        y_normal_max = tf.reduce_max(y_normal)\n",
    "        y_normal_min = tf.reduce_min(y_normal)\n",
    "        #print(\"normal\",normal_index ,'\\n', y_normal ,'\\n', y_normal_max.numpy() ,'\\n', y_normal_min.numpy())\n",
    "        \n",
    "        #print(str(i + BATCH_SIZE))\n",
    "        anomaly_index = tf.random.shuffle(tf.range(32))\n",
    "        y_anomaly = tf.gather(scores[i + BATCH_SIZE], anomaly_index)\n",
    "        y_anomaly_max = tf.reduce_max(y_anomaly)\n",
    "        y_anomaly_min = tf.reduce_min(y_anomaly)\n",
    "        #print(\"abnormal\",anomaly_index ,'\\n', y_anomaly ,'\\n', y_anomaly_max.numpy() ,'\\n', y_anomaly_min.numpy())\n",
    "        \n",
    "        ## original milbert\n",
    "        ## sparsity uses anomaly scores shuffled\n",
    "        ## smooth uses original anomaly scores\n",
    "        loss += tf.nn.relu(1.0 - y_anomaly_max + y_normal_max) \n",
    "        sparsity += tf.reduce_sum(y_anomaly) * lambda1\n",
    "        smooth += tf.reduce_sum(tf.square(scores[i + BATCH_SIZE, :31] - scores[i + BATCH_SIZE, 1:32])) * lambda1\n",
    "        \n",
    "    l = ((loss + sparsity + smooth ) / BATCH_SIZE).numpy()\n",
    "\n",
    "    print(\"l\",l)\n",
    "    return l \n",
    "    \n",
    "    \n",
    "l1 , l11 = loss_deepmil(scores)\n",
    "l2  = loss_milbert(scores)\n",
    "#abs(l1 - l2)\n",
    "print('\\n', np.isclose(l1 , l11 , l2) )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RTFM FEATURES TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = '/raid/DATASETS/anomaly/UCF_Crimes/features/I3D_RTFM_10CROP/train_abnormal/Abuse001_x264_i3d.npy'\n",
    "features = np.load(p1)\n",
    "print(np.shape(features))\n",
    "\n",
    "features = features.transpose(1, 0, 2)  # [10, B, T, F]\n",
    "print(np.shape(features))\n",
    "divided_features = []\n",
    "for feature in features:\n",
    "    feature = process_feat(feature, 32)  # divide a video into 32 segments\n",
    "    print(np.shape(feature))\n",
    "    divided_features.append(feature)\n",
    "    \n",
    "divided_features = np.array(divided_features, dtype=np.float32)\n",
    "print(np.shape(divided_features))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GT RTFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = 'list/gt-ucf_rtfm.npy'\n",
    "features = np.load(p1)\n",
    "print(np.shape(features))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((3 , 10))\n",
    "aa = np.ones_like(a)\n",
    "aaa = np.append(a , aa)\n",
    "\n",
    "b = np.zeros_like(aaa)\n",
    "\n",
    "np.allclose(aaa[:3] , b[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = np.load(globo.UCFCRIME_GT)\n",
    "test_tframes = np.shape(aaa)[0]\n",
    "\n",
    "ooo = open(globo.UCFCRIME_I3D_DEEPMIL_LISTS[\"test\"])\n",
    "alltframes = 0\n",
    "for i , line in enumerate(ooo):\n",
    "    fpath = line.strip('\\n')\n",
    "    feat = np.load(fpath)\n",
    "    tsteps = np.shape(feat)[0]\n",
    "    ftotal = tsteps * 16\n",
    "    \n",
    "    gtfp = aaa[alltframes : alltframes+ftotal]\n",
    "    print(f'{fpath}\\n{ftotal}\\n{gtfp}')\n",
    "    \n",
    "    #if i in (139,140) : print( fpath , tsteps)\n",
    "    alltframes += ftotal\n",
    "    \n",
    "print(alltframes)\n",
    "assert alltframes ==  test_tframes\n",
    "\n",
    "## somehow the i3d rtfm features are incomplete\n",
    "#ooo = open(globo.UCFCRIME_I3D_RTFM_LISTS[\"test\"])\n",
    "#cnt = 0\n",
    "#for line in ooo:\n",
    "#    fn = line.strip('\\n')\n",
    "#    feat = np.load(fn)\n",
    "#    print(feat)\n",
    "#    ts = np.shape(feat)[0]\n",
    "#    cnt += ts*16\n",
    "#assert cnt ==  test_tframes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zugpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
